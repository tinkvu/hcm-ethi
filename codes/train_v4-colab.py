# -*- coding: utf-8 -*-
"""train-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RGr0cxkps0gMWfcX0zL-68FE7cciS5_Q
"""

import os
import pandas as pd
import torch
import wandb
import pdb

# Check GPU availability
print("CUDA Available:", torch.cuda.is_available())

import logging
import pandas as pd
import librosa
import numpy as np
import torch
import wandb
from argparse import ArgumentParser
from pathlib import Path
from transformers import (
    EvalPrediction,
    Trainer,
    TrainingArguments,
    Wav2Vec2CTCTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
)
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass
import evaluate

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

class ReadSpeechDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        df: pd.DataFrame,  # Expecting a DataFrame
        do_awgn: bool = False,
        awgn_snr: float = 0.05,
        do_masking: bool = False,
        max_mask_len: int = 3200,
    ) -> None:
        self.df = df  # Store DataFrame
        self.do_awgn = do_awgn
        self.awgn_snr = awgn_snr
        self.do_masking = do_masking
        self.max_mask_len = max_mask_len

    def _awgn(self, audio: np.ndarray) -> np.ndarray:
        noise = np.random.randn(len(audio))
        audio_power = np.sum(audio**2)
        noise_power = np.sum(noise**2)
        scale = np.sqrt(audio_power / noise_power * 10 ** -(self.awgn_snr / 10))
        audio = audio + scale * noise
        audio = np.clip(audio, -1, 1)
        return audio

    def _masking(self, audio: np.ndarray) -> np.ndarray:
        mask_len = np.random.randint(0, self.max_mask_len)
        mask_start = np.random.randint(0, len(audio) - mask_len)
        audio[mask_start : mask_start + mask_len] = 0
        return audio

    def __len__(self) -> int:
        return len(self.df)

    def __getitem__(self, index: int) -> dict:
        # Ensure the path column is the correct column name in your CSV
        audio_path = self.df.iloc[index]["path"]
        text = self.df.iloc[index]["transcript"]

        if not isinstance(text, str) or text.strip() == "":
            print(f"Warning: Empty or invalid text at index {index}")
            text = " "  # Assign a default space to avoid breaking the processor

        try:
            # Explicitly specify sr and mono
            audio, _ = librosa.load(audio_path, sr=16000, mono=True)
            audio = audio.squeeze()  # Ensure 1D array

            if self.do_awgn:
                audio = self._awgn(audio)
            if self.do_masking:
                audio = self._masking(audio)

            return {"input_values": audio, "labels": text}
        except Exception as e:
            print(f"Error loading audio file {audio_path}: {e}")
            # Return a dummy audio and text to prevent complete failure
            return {"input_values": np.zeros(16000), "labels": ""}

@dataclass
class DataCollatorWav2Vec2:
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    return_tensors: str = "pt"

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        '''
        print("=== DEBUG: FEATURES ===")
        for i, feat in enumerate(features):
            print(f"Sample {i}:")
            print(f"  - input_values type: {type(feat['input_values'])}, shape: {np.array(feat['input_values']).shape}")
            print(f"  - labels type: {type(feat['labels'])}, value: {feat['labels']}")
        '''


        # Directly process the input_values and labels
        # Separate input values and labels (if present)
        input_features = [{"input_values": f["input_values"]} for f in features]
        label_features = [{"input_ids": self.processor.tokenizer(f["labels"]).input_ids} for f in features if "labels" in f]

        #if not all(isinstance(label, str) for label in label_features):
        #    raise ValueError(f"ERROR: Non-string labels detected! Labels: {label_features}")

        # print("Labels before processing:", label_features)
        # print("Labels type:", type(label_features))
        # print("Labels sample:", label_features[:5])

        # Pad inputs
        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors=self.return_tensors,
        )

        # Pad labels if present
        if label_features:
            labels_batch = self.processor.pad(
                labels=label_features,
                padding=self.padding,
                return_tensors=self.return_tensors,
            )
            # Replace padding with -100 to ignore in loss computation
            labels = labels_batch["input_ids"].masked_fill(
                labels_batch["input_ids"] == self.processor.tokenizer.pad_token_id, -100)
            batch["labels"] = labels

        return batch

wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

def compute_metrics(pred: "EvalPrediction", tokenizer: Wav2Vec2CTCTokenizer):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)
    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

    pred_str = tokenizer.batch_decode(pred_ids)
    label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)
    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    print(f"* wer: {wer}, cer: {cer}")
    return {"wer": wer, "cer": cer}

def save_everything(model, processor, output_dir):
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Save model (weights + config)
    model.save_pretrained(output_dir)

    # Save tokenizer + feature extractor (inside processor)
    processor.save_pretrained(output_dir)

    print(f"âœ… Model and processor saved to: {output_dir}")


def main():
    # parser = ArgumentParser()
    # parser.add_argument("--csv_path", type=str, required=True)
    # parser.add_argument("--output_dir", type=str)
    # parser.add_argument("--wandb_proj", type=str)
    # parser.add_argument("--resume_from_checkpoint", type=str)
    # parser.add_argument("--base_model", type=str, default="facebook/wav2vec2-large-xlsr-53")
    # parser.add_argument("--do_awgn", action="store_true")
    # parser.add_argument("--awgn_snr", type=float, default=0.05)
    # parser.add_argument("--do_masking", action="store_true")
    # parser.add_argument("--max_mask_len", type=int, default=3200)
    # parser.add_argument("--batch_size", type=int, default=2)
    # parser.add_argument("--max_steps", type=int, default=300000)
    # parser.add_argument("--save_steps", type=int, default=1000)
    # parser.add_argument("--eval_steps", type=int, default=1000)
    # parser.add_argument("--logging_steps", type=int, default=100)
    # parser.add_argument("--learning_rate", type=float, default=1e-5)
    # parser.add_argument("--warmup_steps", type=int, default=1000)
    # parser.add_argument("--fp16", action="store_true")
    # args = parser.parse_args()



    #New Try
    parser = ArgumentParser()
    parser.add_argument("--csv_path", type=str, required=True)
    parser.add_argument("--output_dir", type=str)
    parser.add_argument("--wandb_proj", type=str)
    parser.add_argument("--resume_from_checkpoint", type=str)
    parser.add_argument("--base_model", type=str, default="facebook/wav2vec2-large-xlsr-53")
    parser.add_argument("--do_awgn", action="store_true")
    parser.add_argument("--awgn_snr", type=float, default=0.05)
    parser.add_argument("--do_masking", action="store_true")
    parser.add_argument("--max_mask_len", type=int, default=3200)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--max_steps", type=int, default=300000)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--eval_steps", type=int, default=1000)
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--warmup_steps", type=int, default=1000)
    parser.add_argument("--fp16", action="store_true")
    # Add these new arguments
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, 
                        help="Number of updates steps to accumulate before performing a backward/update pass")
    parser.add_argument("--gradient_checkpointing", action="store_true", 
                        help="Enable gradient checkpointing to save memory at the expense of slower backward pass")
    parser.add_argument("--eval_batch_size", type=int, default=None, 
                    help="Evaluation batch size. If None, uses the same as training batch size.")
    parser.add_argument("--max_duration", type=float, default=None, 
                        help="Maximum audio duration in seconds. Longer clips will be trimmed")
    args = parser.parse_args()

    wandb.login(key="4912c504dd70a68bd1daa978591aa04ebb5a68da")

    df = pd.read_csv(args.csv_path)
    train_df = df.sample(frac=0.8, random_state=42)
    test_df = df.drop(train_df.index)

    tokenizer = Wav2Vec2CTCTokenizer(vocab_file="/content/vocab-1.json", word_delimiter_token="|")
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(args.base_model)
    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
    model = Wav2Vec2ForCTC.from_pretrained(
    args.base_model,
    vocab_size=tokenizer.vocab_size,
    ignore_mismatched_sizes=True)


    train_dataset = ReadSpeechDataset(train_df, do_awgn=args.do_awgn, do_masking=args.do_masking)
    eval_dataset = ReadSpeechDataset(test_df)

    # Add verbose logging
    logging.basicConfig(level=logging.INFO)

    # Validate dataset
    print("Total training samples:", len(train_df))
    print("Total validation samples:", len(test_df))

    # Check a few audio files
    for _, row in train_df.head().iterrows():
        try:
            audio, _ = librosa.load(row['path'], sr=16000, mono=True)
            print(f"Successfully loaded: {row['path']}")
        except Exception as e:
            print(f"Failed to load: {row['path']} - {e}")

    dataset = ReadSpeechDataset(train_df)
    collator = DataCollatorWav2Vec2(processor)

    sample_batch = [dataset[i] for i in range(2)]  # Fetch 2 samples
    collator(sample_batch)  

    trainer = Trainer(
        model=model,
        args=TrainingArguments(output_dir=args.output_dir,num_train_epochs=5,per_device_train_batch_size=1,gradient_accumulation_steps=8,fp16=True,per_device_eval_batch_size=1 if args.eval_batch_size is None else args.eval_batch_size, gradient_checkpointing=args.gradient_checkpointing),
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorWav2Vec2(processor),
        compute_metrics=lambda x: compute_metrics(x, tokenizer),
    )
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    # trainer.save_model(args.output_dir + "/final_model")
    save_everything(model, processor, args.output_dir)
    import torch
    if torch.cuda.is_available():
      torch.cuda.empty_cache()
    
    trainer.evaluate()
    # trainer.save_model(args.output_dir)



if __name__ == "__main__":
    main()
